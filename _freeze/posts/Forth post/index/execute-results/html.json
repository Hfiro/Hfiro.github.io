{
  "hash": "0f94c588ec479fd4ecfb05f63f48fdfe",
  "result": {
    "markdown": "---\ntitle: Classification\nauthor: Hoda Alemrajabi\ndate: '2023-11-06'\ncategories:\n  - news\n  - code\n  - analysis\n  - data visualization\nimage: forth post.png\n---\n\nThis post is about classification in ML!\n\n![](forth post.png){width=\"366\"}\n\nIn this post, I talked about Receiver Operating Characteristic (ROC) curve which is an important tool used in classification problems to evaluate the performance of a classifier. I created a Python example to visualize an ROC curve using a simple classification model.\n\nThese are the steps:\n\n-   **Generate Synthetic Data**: Creating a binary classification dataset.\n\n-   **Build a Classifier**: Using a simple classifier like logistic regression.\n\n-   **Compute ROC Curve**: Using **`roc_curve`** from **`sklearn.metrics`** to compute the true positive rate and false positive rate at various threshold settings.\n\n-   **Visualization**: Plotting the ROC curve.\n\nHere's the Python code for this:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\n\n# Generate synthetic binary classification dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=0)\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# Build and train a logistic regression classifier\nclassifier = LogisticRegression()\nclassifier.fit(X_train, y_train)\n\n# Predict probabilities\ny_probs = classifier.predict_proba(X_test)\n\n# Compute ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_probs[:, 1])\n\n# Compute Area Under the Curve (AUC)\nroc_auc = auc(fpr, tpr)\n\n# Visualization\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=600 height=449}\n:::\n:::\n\n\nRunning this code displays an ROC curve, which is a graphical representation of the trade-off between the true positive rate and false positive rate at various thresholds. The AUC value provides an aggregate measure of the classifier's performance across all possible classification thresholds.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}