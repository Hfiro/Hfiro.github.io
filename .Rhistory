copy NUL .nojekyll
reticulate::repl_python()
title: "Post With Code"
author: "Hoda Alemrajabi"
date: "2023-11-06"
categories: [news, code, analysis]
image: "image.jpg"
This is a post with executable code.
1 + 1
title: "Post With Code"
author: "Hoda Alemrajabi"
date: "2023-11-06"
categories: ["news, code, analysis"]
image: "image.jpg"
"This is a post with executable code."
1 + 1
reticulate::repl_python()
import numpy as np
import matplotlib.pyplot as plt
# Generate random data from a normal distribution
mu, sigma = 0, 0.1  # mean and standard deviation
s = np.random.normal(mu, sigma, 1000)
# Create the histogram of the data
count, bins, ignored = plt.hist(s, 30, density=True)
# Plot the distribution curve
plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *
np.exp(- (bins - mu)**2 / (2 * sigma**2)),
linewidth=2, color='r')
plt.title('Normal Distribution (Gaussian)')
plt.xlabel('Value')
plt.ylabel('Probability Density')
plt.show()
quit
reticulate::repl_python()
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
# Generate random data
n_samples = 300
n_features = 2
centers = 4
X, y_true = make_blobs(n_samples=n_samples, n_features=n_features, centers=centers)
# Apply K-Means Clustering
kmeans = KMeans(n_clusters=centers)
kmeans.fit(X)
y_kmeans = kmeans.predict(X)
# Visualization
plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')
centers = kmeans.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)
plt.title('K-Means Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
# Generate synthetic data
np.random.seed(0)
X = 2 - 3 * np.random.normal(0, 1, 20)
y = X - 2 * (X ** 2) + np.random.normal(-3, 3, 20)
# Reshaping for model
X = X[:, np.newaxis]
# Linear Regression
linear_regressor = LinearRegression()
linear_regressor.fit(X, y)
y_pred_linear = linear_regressor.predict(X)
# Nonlinear Regression (Polynomial)
degree = 2
polyreg = make_pipeline(PolynomialFeatures(degree), LinearRegression())
polyreg.fit(X, y)
y_pred_poly = polyreg.predict(X)
# Visualization
plt.scatter(X, y, color='black', label='Data')
plt.plot(X, y_pred_linear, color='blue', label='Linear Regression')
plt.plot(X, y_pred_poly, color='red', label='Nonlinear Regression')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Linear vs Nonlinear Regression')
plt.legend()
plt.show()
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
# Generate synthetic data
np.random.seed(0)
X = 2 - 3 * np.random.normal(0, 1, 20)
y = X - 2 * (X ** 2) + np.random.normal(-3, 3, 20)
# Reshaping and sorting the data based on X
X = X[:, np.newaxis]
sort_index = X.flatten().argsort()  # Get the sorted order of indices
X = X[sort_index]
y = y[sort_index]
# Linear Regression
linear_regressor = LinearRegression()
linear_regressor.fit(X, y)
y_pred_linear = linear_regressor.predict(X)
# Nonlinear Regression (Polynomial)
degree = 2
polyreg = make_pipeline(PolynomialFeatures(degree), LinearRegression())
polyreg.fit(X, y)
y_pred_poly = polyreg.predict(X)
# Visualization
plt.scatter(X, y, color='black', label='Data')
plt.plot(X, y_pred_linear, color='blue', label='Linear Regression')
plt.plot(X, y_pred_poly, color='red', label='Nonlinear Regression')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Linear vs Nonlinear Regression')
plt.legend()
plt.show()
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
# Generate synthetic data
np.random.seed(0)
X = 2 - 3 * np.random.normal(0, 1, 20)
y = X - 2 * (X ** 2) + np.random.normal(-3, 3, 20)
# Reshaping and sorting the data based on X
X = X[:, np.newaxis]
sort_index = X.flatten().argsort()  # Get the sorted order of indices
X = X[sort_index]
y = y[sort_index]
# Linear Regression
linear_regressor = LinearRegression()
linear_regressor.fit(X, y)
y_pred_linear = linear_regressor.predict(X)
# Nonlinear Regression (Polynomial)
degree = 2
polyreg = make_pipeline(PolynomialFeatures(degree), LinearRegression())
polyreg.fit(X, y)
y_pred_poly = polyreg.predict(X)
# Visualization
plt.scatter(X, y, color='black', label='Data')
plt.plot(X, y_pred_linear, color='blue', label='Linear Regression')
plt.plot(X, y_pred_poly, color='red', label='Nonlinear Regression')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Linear vs Nonlinear Regression')
plt.legend()
plt.show()
