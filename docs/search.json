[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "This is the first post in the Quarto blog of Hoda Alemrajabi . Welcome!\n\nIn this post, I will talk about Probability theory and random variables.\nIn this topic, one common concept is the visualization of different probability distributions, which are fundamental in understanding random variables. Here’s an example where I visualized a Normal (Gaussian) distribution, which is a cornerstone in probability theory and often used in machine learning:\n\nGenerating Random Data: Using numpy to generate random data following a normal distribution.\nVisualization: Using matplotlib to plot the histogram of this data to visualize the distribution.\n\nHere’s the Python code to do this:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate random data from a normal distribution\nmu, sigma = 0, 0.1  # mean and standard deviation\ns = np.random.normal(mu, sigma, 1000)\n\n# Create the histogram of the data\ncount, bins, ignored = plt.hist(s, 30, density=True)\n\n# Plot the distribution curve\nplt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *\n               np.exp(- (bins - mu)**2 / (2 * sigma**2)),\n         linewidth=2, color='r')\nplt.title('Normal Distribution (Gaussian)')\nplt.xlabel('Value')\nplt.ylabel('Probability Density')\nplt.show()\n\n\n\n\nRunning this code will give us a graphical representation of a normal distribution, which is an essential concept in probability theory related to random variables. We can modify mu and sigma to see how the distribution changes. This kind of visualization is particularly useful in understanding data distributions in machine learning."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hfiro.github.io",
    "section": "",
    "text": "Classification\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2023\n\n\nHoda Alemrajabi\n\n\n\n\n\n\n  \n\n\n\n\nLinear and Nonlinear Regression\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2023\n\n\nHoda Alemrajabi\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nHoda Alemrajabi\n\n\n\n\n\n\n  \n\n\n\n\nProbability Theory and Random Variables\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2023\n\n\nHoda Alemrajabi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Clustering",
    "section": "",
    "text": "This post is about clustering in ML!\n\nClustering is a crucial technique in machine learning, particularly in unsupervised learning. A common method for clustering is the K-Means algorithm. We can visualize clustering by using Python with libraries such as matplotlib for plotting and sklearn for machine learning functionalities.\nHere’s an example of a Python script that generates random data, applies the K-Means clustering algorithm, and visualizes the results:\n\nGenerate Random Data: Creating a dataset with random points.\nApply K-Means Clustering: Using the K-Means algorithm to identify clusters in the data.\nVisualization: Plotting the data points and color them according to their cluster.\n\nHere’s the Python code:\n\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\n# Generate random data\nn_samples = 300\nn_features = 2\ncenters = 4\nX, y_true = make_blobs(n_samples=n_samples, n_features=n_features, centers=centers)\n\n# Apply K-Means Clustering\nkmeans = KMeans(n_clusters=centers)\nkmeans.fit(X)\ny_kmeans = kmeans.predict(X)\n\n# Visualization\nplt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)\nplt.title('K-Means Clustering')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.show()\n\nC:\\Users\\hodaa\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\hodaa\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n\n\n\n\n\n\nRunning this code will give usa visual representation of the clustered data points, showing how K-Means algorithm groups the data into distinct clusters. This is a simple yet effective way to understand the concept of clustering in machine learning."
  },
  {
    "objectID": "posts/post-with-code/Untitled.html",
    "href": "posts/post-with-code/Untitled.html",
    "title": "Hfiro.github.io",
    "section": "",
    "text": "%pip install jupytext\n\nCollecting jupytext\n  Obtaining dependency information for jupytext from https://files.pythonhosted.org/packages/9e/e3/3c5b6cce216d090ed6cf6cc258602f836d050738ac02f97cb71675f9cfe3/jupytext-1.15.2-py3-none-any.whl.metadata\n  Downloading jupytext-1.15.2-py3-none-any.whl.metadata (9.7 kB)\nRequirement already satisfied: nbformat in c:\\users\\hodaa\\anaconda3\\lib\\site-packages (from jupytext) (5.7.0)\nRequirement already satisfied: pyyaml in c:\\users\\hodaa\\anaconda3\\lib\\site-packages (from jupytext) (6.0)\nRequirement already satisfied: toml in c:\\users\\hodaa\\anaconda3\\lib\\site-packages (from jupytext) (0.10.2)\nRequirement already satisfied: markdown-it-py&gt;=1.0.0 in c:\\users\\hodaa\\anaconda3\\lib\\site-packages (from jupytext) (2.2.0)\nRequirement already satisfied: mdit-py-plugins in c:\\users\\hodaa\\anaconda3\\lib\\site-packages (from jupytext) (0.3.0)\nRequirement already satisfied: mdurl~=0.1 in c:\\users\\hodaa\\anaconda3\\lib\\site-packages (from markdown-it-py&gt;=1.0.0-&gt;jupytext) (0.1.0)\nRequirement already satisfied: fastjsonschema in c:\\users\\hodaa\\anaconda3\\lib\\site-packages (from nbformat-&gt;jupytext) (2.16.2)\nRequirement already satisfied: jsonschema&gt;=2.6 in c:\\users\\hodaa\\anaconda3\\lib\\site-packages (from nbformat-&gt;jupytext) (4.17.3)\nRequirement already satisfied: jupyter-core in c:\\users\\hodaa\\anaconda3\\lib\\site-packages (from nbformat-&gt;jupytext) (5.3.0)\nRequirement already satisfied: traitlets&gt;=5.1 in c:\\users\\hodaa\\anaconda3\\lib\\site-packages (from nbformat-&gt;jupytext) (5.7.1)\nRequirement already satisfied: attrs&gt;=17.4.0 in c:\\users\\hodaa\\anaconda3\\lib\\site-packages (from jsonschema&gt;=2.6-&gt;nbformat-&gt;jupytext) (22.1.0)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,&gt;=0.14.0 in c:\\users\\hodaa\\anaconda3\\lib\\site-packages (from jsonschema&gt;=2.6-&gt;nbformat-&gt;jupytext) (0.18.0)\nRequirement already satisfied: platformdirs&gt;=2.5 in c:\\users\\hodaa\\anaconda3\\lib\\site-packages (from jupyter-core-&gt;nbformat-&gt;jupytext) (2.5.2)\nRequirement already satisfied: pywin32&gt;=300 in c:\\users\\hodaa\\anaconda3\\lib\\site-packages (from jupyter-core-&gt;nbformat-&gt;jupytext) (305.1)\nDownloading jupytext-1.15.2-py3-none-any.whl (307 kB)\n   ---------------------------------------- 0.0/307.2 kB ? eta -:--:--\n   - -------------------------------------- 10.2/307.2 kB ? eta -:--:--\n   - -------------------------------------- 10.2/307.2 kB ? eta -:--:--\n   ------- ------------------------------- 61.4/307.2 kB 409.6 kB/s eta 0:00:01\n   ------------- ------------------------ 112.6/307.2 kB 656.4 kB/s eta 0:00:01\n   --------------------- ---------------- 174.1/307.2 kB 807.1 kB/s eta 0:00:01\n   ------------------------------- ------ 256.0/307.2 kB 983.0 kB/s eta 0:00:01\n   -------------------------------------- 307.2/307.2 kB 999.9 kB/s eta 0:00:00\nInstalling collected packages: jupytext\nSuccessfully installed jupytext-1.15.2\nNote: you may need to restart the kernel to use updated packages."
  },
  {
    "objectID": "posts/Third post/index.html",
    "href": "posts/Third post/index.html",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "This post is about linear and nonlinear regression in ML!\n\nLinear and nonlinear regression are important concepts in machine learning for modeling relationships between variables. In this post, I created an example in Python to visualize both linear and nonlinear regression.\nI used the following approach:\n\nGenerate Synthetic Data: Creating a dataset that has a nonlinear relationship.\nLinear Regression: Applying a linear regression model to this data.\nNonlinear Regression: Applying a nonlinear regression model, like polynomial regression.\nVisualization: Plotting both the linear and nonlinear regression models along with the data.\n\nHere’s the Python code for this:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\n# Generate synthetic data\nnp.random.seed(0)\nX = 2 - 3 * np.random.normal(0, 1, 20)\ny = X - 2 * (X ** 2) + np.random.normal(-3, 3, 20)\n\n# Reshaping and sorting the data based on X\nX = X[:, np.newaxis]\nsort_index = X.flatten().argsort()  # Get the sorted order of indices\nX = X[sort_index]\ny = y[sort_index]\n\n# Linear Regression\nlinear_regressor = LinearRegression()\nlinear_regressor.fit(X, y)\ny_pred_linear = linear_regressor.predict(X)\n\n# Nonlinear Regression (Polynomial)\ndegree = 2\npolyreg = make_pipeline(PolynomialFeatures(degree), LinearRegression())\npolyreg.fit(X, y)\ny_pred_poly = polyreg.predict(X)\n\n# Visualization\nplt.scatter(X, y, color='black', label='Data')\nplt.plot(X, y_pred_linear, color='blue', label='Linear Regression')\nplt.plot(X, y_pred_poly, color='red', label='Nonlinear Regression')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Linear vs Nonlinear Regression')\nplt.legend()\nplt.show()\n\n\n\n\nRunning this code will show us a plot where you can see how linear regression fails to capture the complexity of the data, while the polynomial regression provides a better fit, illustrating the concept of linear and nonlinear regression in machine learning."
  },
  {
    "objectID": "posts/Forth post/index.html",
    "href": "posts/Forth post/index.html",
    "title": "Classification",
    "section": "",
    "text": "This post is about classification in ML!\n\nIn this post, I talked about Receiver Operating Characteristic (ROC) curve which is an important tool used in classification problems to evaluate the performance of a classifier. I created a Python example to visualize an ROC curve using a simple classification model.\nThese are the steps:\n\nGenerate Synthetic Data: Creating a binary classification dataset.\nBuild a Classifier: Using a simple classifier like logistic regression.\nCompute ROC Curve: Using roc_curve from sklearn.metrics to compute the true positive rate and false positive rate at various threshold settings.\nVisualization: Plotting the ROC curve.\n\nHere’s the Python code for this:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\n\n# Generate synthetic binary classification dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=0)\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# Build and train a logistic regression classifier\nclassifier = LogisticRegression()\nclassifier.fit(X_train, y_train)\n\n# Predict probabilities\ny_probs = classifier.predict_proba(X_test)\n\n# Compute ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_probs[:, 1])\n\n# Compute Area Under the Curve (AUC)\nroc_auc = auc(fpr, tpr)\n\n# Visualization\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\nRunning this code displays an ROC curve, which is a graphical representation of the trade-off between the true positive rate and false positive rate at various thresholds. The AUC value provides an aggregate measure of the classifier’s performance across all possible classification thresholds."
  }
]