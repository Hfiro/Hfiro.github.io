---
title: Classification
author: Hoda Alemrajabi
date: '2023-11-06'
categories:
  - news
  - code
  - analysis
  - data visualization
image: forth post.png
jupyter:
  jupytext:
    formats: 'ipynb,qmd'
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

This post is about classification in ML!

![](forth%20post.png){width="366"}

In this post, I talked about Receiver Operating Characteristic (ROC) curve which is an important tool used in classification problems to evaluate the performance of a classifier. I created a Python example to visualize an ROC curve using a simple classification model.

These are the steps:

-   **Generate Synthetic Data**: Creating a binary classification dataset.

-   **Build a Classifier**: Using a simple classifier like logistic regression.

-   **Compute ROC Curve**: Using **`roc_curve`** from **`sklearn.metrics`** to compute the true positive rate and false positive rate at various threshold settings.

-   **Visualization**: Plotting the ROC curve.

Here's the Python code for this:

```{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve, auc

# Generate synthetic binary classification dataset
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=0)

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# Build and train a logistic regression classifier
classifier = LogisticRegression()
classifier.fit(X_train, y_train)

# Predict probabilities
y_probs = classifier.predict_proba(X_test)

# Compute ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_probs[:, 1])

# Compute Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Visualization
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()
```

Running this code displays an ROC curve, which is a graphical representation of the trade-off between the true positive rate and false positive rate at various thresholds. The AUC value provides an aggregate measure of the classifier's performance across all possible classification thresholds.
